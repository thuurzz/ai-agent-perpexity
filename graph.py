from langgraph.graph import START, END, StateGraph
from langgraph.types import Send
from langchain_openai import ChatOpenAI

from pydantic import BaseModel
from schemas import *
from prompt import *
from dotenv import load_dotenv
from tavily import TavilyClient
import logging

from datetime import datetime
from pdf_generator import generate_report_files

# Configurar logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()
logger.info("ğŸš€ AplicaÃ§Ã£o iniciada e variÃ¡veis de ambiente carregadas")

# LLMs
logger.info("ğŸ¤– Inicializando LLMs...")
llm = ChatOpenAI(model_name="gpt-4o-mini")
reasoning_llm = ChatOpenAI(model_name="o3-mini")
logger.info("âœ… LLMs inicializados com sucesso")

# NÃ³s


def build_first_queries(state: ReportState) -> ReportState:
    logger.info("ğŸ” Iniciando build_first_queries...")
    logger.info(f"ğŸ“ Estado recebido: {state}")

    class QueryList(BaseModel):
        queries: List[str]

    user_input = state.user_input
    logger.info(f"ğŸ‘¤ Input do usuÃ¡rio: {user_input}")

    prompt = build_queries.format(user_input=user_input)
    logger.info(f"ğŸ“‹ Prompt gerado: {prompt[:100]}...")

    query_llm = llm.with_structured_output(QueryList)
    logger.info("ğŸ”„ Enviando prompt para LLM...")

    response = query_llm.invoke(prompt)
    logger.info(f"ğŸ“Š Resposta do LLM: {response}")

    state.queries = response.queries
    logger.info(f"âœ… Queries geradas: {state.queries}")
    logger.info(f"ğŸ“¤ Estado final: {state}")

    return state


def single_search(query: str):
    logger.info(f"ğŸ” Iniciando busca para query: {query}")

    tavily_client = TavilyClient()
    logger.info("ğŸŒ Cliente Tavily inicializado")

    results = tavily_client.search(
        query, max_results=1, include_raw_content=False)
    logger.info(
        f"ğŸ“‹ Resultados da busca: {len(results.get('results', []))} resultado(s)")

    query_results = []
    for i, result in enumerate(results["results"]):
        logger.info(f"ğŸ“„ Processando resultado {i+1}: {result['title']}")
        url = result["url"]
        logger.info(f"ğŸ”— URL: {url}")

        url_extraction = tavily_client.extract(url)
        if len(url_extraction["results"]) > 0:
            raw_content = url_extraction["results"][0]["raw_content"]
            logger.info(f"ğŸ“ ConteÃºdo extraÃ­do: {len(raw_content)} caracteres")

            prompt = resume_search.format(user_input=query,  # Corrigido: usar query em vez de user_input
                                          search_results=raw_content)
            logger.info("ğŸ¤– Enviando para LLM para resumo...")

            llm_result = llm.invoke(prompt)
            logger.info(
                f"âœ… Resumo gerado: {len(llm_result.content)} caracteres")

            query_results += [QueryResult(title=result["title"],
                                          url=url,
                                          resume=llm_result.content)]
        else:
            logger.warning(f"âš ï¸ NÃ£o foi possÃ­vel extrair conteÃºdo de {url}")

    logger.info(f"ğŸ¯ Total de resultados processados: {len(query_results)}")
    return {"queries_results": query_results}


def spawn_researchers(state: ReportState):
    logger.info(
        f"ğŸ‘¥ Iniciando spawn_researchers com {len(state.queries)} queries")
    logger.info(f"ğŸ“‹ Queries: {state.queries}")

    sends = [Send("single_search", query) for query in state.queries]
    logger.info(f"ğŸš€ Criando {len(sends)} tarefas de busca paralela")

    return sends


def final_writer(state: ReportState):
    logger.info("âœï¸ Iniciando final_writer...")
    logger.info(
        f"ğŸ“Š Estado recebido: queries_results = {len(state.queries_results)} resultados")

    search_results = ""
    references = ""
    for i, result in enumerate(state.queries_results):
        logger.info(f"ğŸ“„ Processando resultado {i+1}: {result.title}")
        search_results += f"[{i+1}]\n\n"
        search_results += f"Title: {result.title}\n"
        search_results += f"URL: {result.url}\n"
        search_results += f"Content: {result.resume}\n"
        search_results += f"================\n\n"

        references += f"[{i+1}] - [{result.title}]({result.url})\n"

    logger.info(f"ğŸ“ ConteÃºdo compilado: {len(search_results)} caracteres")
    logger.info(f"ğŸ”— ReferÃªncias: {len(references)} caracteres")

    prompt = build_final_response.format(user_input=state.user_input,  # Corrigido: usar state.user_input
                                         search_results=search_results)
    logger.info("ğŸ¤– Enviando para LLM de reasoning...")

    llm_result = reasoning_llm.invoke(prompt)
    logger.info(
        f"âœ… Resposta final gerada: {len(llm_result.content)} caracteres")

    final_response = llm_result.content + "\n\n References:\n" + references
    logger.info(f"ğŸ“‹ Resposta final completa: {len(final_response)} caracteres")

    # Gerar PDF e Markdown usando o mÃ³dulo dedicado
    try:
        logger.info("ğŸ“„ Gerando relatÃ³rio profissional (PDF + Markdown)...")
        
        # Usar o mÃ³dulo de geraÃ§Ã£o de PDF com o user_input para nome do arquivo
        report_files = generate_report_files(final_response, user_input=state.user_input)
        
        logger.info(f"âœ… PDF profissional: {report_files['pdf_path']}")
        logger.info(f"ğŸ“ Arquivo Markdown: {report_files['markdown_path']}")
        
    except Exception as e:
        logger.error(f"âŒ Erro ao gerar relatÃ³rio: {str(e)}")
        logger.error(f"ğŸ” Tipo do erro: {type(e).__name__}")
        # Continuar execuÃ§Ã£o mesmo se a geraÃ§Ã£o falhar

    return {"final_response": final_response}


# Criando o grafo de estados com nÃ³s e arestas
logger.info("ğŸ—ï¸ Construindo o grafo de estados...")
builder = StateGraph(ReportState)

logger.info("â• Adicionando nÃ³s...")
builder.add_node("build_first_queries", build_first_queries)
builder.add_node("single_search", single_search)
builder.add_node("final_writer", final_writer)

logger.info("ğŸ”— Adicionando arestas...")
builder.add_edge(START, "build_first_queries")
builder.add_conditional_edges("build_first_queries",
                              spawn_researchers,
                              ["single_search"])
builder.add_edge("single_search", "final_writer")
builder.add_edge("final_writer", END)

logger.info("âš™ï¸ Compilando o grafo...")
graph = builder.compile()
logger.info("âœ… Grafo compilado com sucesso")


if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("ğŸ¯ INICIANDO EXECUÃ‡ÃƒO PRINCIPAL")
    logger.info("=" * 60)

    user_input = input(
        "ğŸ’¬ Por favor, insira o tÃ³pico para pesquisa e relatÃ³rio: ")
    logger.info(f"ğŸ’¬ Input do usuÃ¡rio: {user_input}")

    initial_state = {"user_input": user_input}
    logger.info(f"ğŸ Estado inicial: {initial_state}")

    try:
        logger.info("ğŸš€ Invocando o grafo...")
        result = graph.invoke(initial_state)
        logger.info(f"âœ… ExecuÃ§Ã£o concluÃ­da com sucesso!")
        logger.info(f"ğŸ“Š Tipo do resultado: {type(result)}")
        logger.info(
            f"ğŸ“‹ Chaves do resultado: {result.keys() if isinstance(result, dict) else 'NÃ£o Ã© dict'}")

        # O PDF e Markdown jÃ¡ foram gerados na funÃ§Ã£o final_writer
        logger.info("âœ… ExecuÃ§Ã£o concluÃ­da! Arquivos gerados:")
        logger.info("ğŸ“„ PDF profissional salvo em: reports/")
        logger.info("ğŸ“ Arquivo Markdown salvo em: reports/")
        logger.info("ğŸ¯ Processo completo finalizado com sucesso!")

    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"âŒ ERRO DURANTE A EXECUÃ‡ÃƒO: {str(e)}")
        logger.error(f"ğŸ” Tipo do erro: {type(e).__name__}")
        logger.error("=" * 60)
        raise
